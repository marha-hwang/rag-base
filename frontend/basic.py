# from langchain.llms import OpenAI  <-- (X) êµ¬ë²„ì „
from langchain_openai import ChatOpenAI # <-- (O) ìµœì‹  ë²„ì „ (ChatOpenAI ì‚¬ìš©)
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from dotenv import load_dotenv
import streamlit as st

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

st.title("ðŸ’¬ My GPT-like Chat")

# 
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- ìˆ˜ì •ëœ ë¶€ë¶„ ---
# 1. ChatOpenAIë¥¼ ì‚¬ìš©í•´ì•¼ 'gpt-3.5-turbo'ë‚˜ 'gpt-4' ê°™ì€ ëŒ€í™”í˜• ëª¨ë¸ì´ í˜¸ì¶œë©ë‹ˆë‹¤.
# 2. streaming=Trueë¡œ ì„¤ì •í•´ì•¼ ê¸€ìžê°€ ì¨ì§€ëŠ” íš¨ê³¼ë¥¼ ë³¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
llm = ChatOpenAI(
    model="gpt-4o",  # ë˜ëŠ” "gpt-3.5-turbo"
    temperature=0, 
    streaming=True
)
# ------------------

if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        st_callback = StreamlitCallbackHandler(st.container())
        
        # invokeì— ë¬¸ìžì—´(prompt)ì„ ë°”ë¡œ ë„£ì–´ë„ ì•Œì•„ì„œ HumanMessageë¡œ ë³€í™˜í•´ ì¤ë‹ˆë‹¤.
        response = llm.invoke(prompt, config={"callbacks": [st_callback]})
        
        # responseëŠ” ê°ì²´(AIMessage)ì´ë¯€ë¡œ .contentë¥¼ ë½‘ì•„ì„œ ì €ìž¥í•´ì•¼ ê¹”ë”í•©ë‹ˆë‹¤.
        st.session_state.messages.append({"role": "assistant", "content": response.content})
        st.markdown(response.content) # StreamlitCallbackHandlerê°€ ìžˆì–´ë„ ìµœì¢… ê²°ê³¼ëŠ” í•œë²ˆ ë” ì°ì–´ì£¼ëŠ” ê²Œ ì•ˆì „í•¨